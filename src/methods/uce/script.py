import os
import pickle
import sys
import tarfile
import tempfile
import zipfile
from argparse import Namespace

import anndata as ad
import numpy as np
import pandas as pd
import sklearn.linear_model
import torch
from accelerate import Accelerator

# Code has hardcoded paths that only work correctly inside the UCE directory
if os.path.isdir("UCE"):
    # For executable we can work inside the UCE directory
    os.chdir("UCE")
else:
    # For Nextflow we need to copy files to the Nextflow working directory
    print(">>> Copying UCE files to local directory...", flush=True)
    import shutil

    shutil.copytree("/workspace/UCE", ".", dirs_exist_ok=True)

# Append current directory to import UCE functions
sys.path.append(".")
from data_proc.data_utils import (
    adata_path_to_prot_chrom_starts,
    get_spec_chrom_csv,
    get_species_to_pe,
    process_raw_anndata,
)
from evaluate import run_eval

## VIASH START
# Note: this section is auto-generated by viash at runtime. To edit it, make changes
# in config.vsh.yaml and then run `viash config inject config.vsh.yaml`.
par = {
    "input_train": "resources_test/task_label_projection/cxg_immune_cell_atlas/train.h5ad",
    "input_test": "resources_test/task_label_projection/cxg_immune_cell_atlas/test.h5ad",
    "output": "output.h5ad",
    "model": "uce-model-v5.zip",
}
meta = {"name": "uce"}
## VIASH END

print("====== UCE ======", flush=True)

print("\n>>> Reading training data...", flush=True)
print(f"Training H5AD file: '{par['input_train']}'", flush=True)
input_train = ad.read_h5ad(par["input_train"])
print(input_train, flush=True)

if input_train.uns["dataset_organism"] == "homo_sapiens":
    species = "human"
elif input_train.uns["dataset_organism"] == "mus_musculus":
    species = "mouse"
else:
    raise ValueError(
        f"Species '{input_train.uns['dataset_organism']}' not yet implemented"
    )

print("\n>>> Creating working directory...", flush=True)
work_dir = tempfile.TemporaryDirectory()
print(f"Working directory: '{work_dir.name}'", flush=True)

print("\n>>> Getting model files...", flush=True)
if os.path.isdir(par["model"]):
    model_temp = None
    model_dir = par["model"]
else:
    model_temp = tempfile.TemporaryDirectory()
    model_dir = model_temp.name

    if zipfile.is_zipfile(par["model"]):
        print("Extracting UCE model from .zip...", flush=True)
        with zipfile.ZipFile(par["model"], "r") as zip_file:
            zip_file.extractall(model_dir)
    elif tarfile.is_tarfile(par["model"]) and par["model"].endswith(".tar.gz"):
        print("Extracting model from .tar.gz...", flush=True)
        with tarfile.open(par["model"], "r:gz") as tar_file:
            tar_file.extractall(model_dir)
            model_dir = os.path.join(model_dir, os.listdir(model_dir)[0])
    else:
        raise ValueError(
            "The 'model' argument should be a directory a .zip file or a .tar.gz file"
        )

print(f"Model directory: '{model_dir}'", flush=True)

print("Extracting protein embeddings...", flush=True)
with tarfile.open(
    os.path.join(model_dir, "protein_embeddings.tar.gz"), "r:gz"
) as tar_file:
    tar_file.extractall("./model_files")
protein_embeddings_dir = os.path.join("./model_files", "protein_embeddings")
print(f"Protein embeddings directory: '{protein_embeddings_dir}'", flush=True)

# # The following sections implement methods in the UCE.evaluate.AnndataProcessor
# # class due to the object not being compatible with the Open Problems setup
model_args = {
    "dir": work_dir.name + "/",
    "skip": True,
    "filter": False,  # Turn this off to get embedding for all cells
    "name": "input",
    "offset_pkl_path": os.path.join(model_dir, "species_offsets.pkl"),
    "spec_chrom_csv_path": os.path.join(model_dir, "species_chrom.csv"),
    "pe_idx_path": os.path.join(work_dir.name, "input_pe_row_idxs.pt"),
    "chroms_path": os.path.join(work_dir.name, "input_chroms.pkl"),
    "starts_path": os.path.join(work_dir.name, "input_starts.pkl"),
}

# AnndataProcessor.preprocess_anndata()
print("\n>>> Preprocessing training data...", flush=True)
# Set X to counts
input_train.X = input_train.layers["counts"]
# Set var names to gene symbols
input_train.var_names = input_train.var["feature_name"]
input_train.write_h5ad(os.path.join(model_args["dir"], "input.h5ad"))

row = pd.Series()
row.path = "input.h5ad"
row.covar_col = np.nan
row.species = species

processed_train, num_cells, num_genes = process_raw_anndata(
    row=row,
    h5_folder_path=model_args["dir"],
    npz_folder_path=model_args["dir"],
    scp="",
    skip=model_args["skip"],
    additional_filter=model_args["filter"],
    root=model_args["dir"],
)
print(processed_train, flush=True)

# AnndataProcessor.generate_idxs()
print("\n>>> Generating training data indexes...", flush=True)
species_to_pe = get_species_to_pe(protein_embeddings_dir)
with open(model_args["offset_pkl_path"], "rb") as f:
    species_to_offsets = pickle.load(f)
gene_to_chrom_pos = get_spec_chrom_csv(model_args["spec_chrom_csv_path"])
spec_pe_genes = list(species_to_pe[species].keys())
offset = species_to_offsets[species]
pe_row_idxs, dataset_chroms, dataset_pos = adata_path_to_prot_chrom_starts(
    processed_train, species, spec_pe_genes, gene_to_chrom_pos, offset
)
torch.save({model_args["name"]: pe_row_idxs}, model_args["pe_idx_path"])
with open(model_args["chroms_path"], "wb+") as f:
    pickle.dump({model_args["name"]: dataset_chroms}, f)
with open(model_args["starts_path"], "wb+") as f:
    pickle.dump({model_args["name"]: dataset_pos}, f)

# AnndataProcessor.run_evaluation()
print("\n>>> Embedding training data...", flush=True)
model_parameters = Namespace(
    token_dim=5120,
    d_hid=5120,
    nlayers=33,  # Small model = 4, full model = 33
    output_dim=1280,
    multi_gpu=False,
    token_file=os.path.join(model_dir, "all_tokens.torch"),
    dir=model_args["dir"],
    pad_length=1536,
    sample_size=1024,
    cls_token_idx=3,
    CHROM_TOKEN_OFFSET=143574,
    chrom_token_right_idx=2,
    chrom_token_left_idx=1,
    pad_token_idx=0,
)

if model_parameters.nlayers == 4:
    model_parameters.model_loc = os.path.join(model_dir, "4layer_model.torch")
    model_parameters.batch_size = 100
else:
    model_parameters.model_loc = os.path.join(model_dir, "33l_8ep_1024t_1280.torch")
    model_parameters.batch_size = 25

accelerator = Accelerator(project_dir=model_args["dir"])
accelerator.wait_for_everyone()
shapes_dict = {model_args["name"]: (num_cells, num_genes)}
run_eval(
    adata=processed_train,
    name=model_args["name"],
    pe_idx_path=model_args["pe_idx_path"],
    chroms_path=model_args["chroms_path"],
    starts_path=model_args["starts_path"],
    shapes_dict=shapes_dict,
    accelerator=accelerator,
    args=model_parameters,
)
embedded_train = ad.read_h5ad(os.path.join(model_args["dir"], "input_uce_adata.h5ad"))
print(embedded_train, flush=True)

print("\n>>> Training logistic regression classifier...", flush=True)
classifier = sklearn.linear_model.LogisticRegression()
classifier.fit(embedded_train.obsm["X_uce"], embedded_train.obs["label"].astype(str))

print("\n>>> Reading test data...", flush=True)
print(f"Test H5AD file: '{par['input_test']}'", flush=True)
input_test = ad.read_h5ad(par["input_test"])
print(input_test, flush=True)

print("\n>>> Preprocessing test data...", flush=True)
input_test.X = input_test.layers["counts"]
input_test.var_names = input_test.var["feature_name"]
input_test.write_h5ad(os.path.join(model_args["dir"], "input_test.h5ad"))

row_test = pd.Series()
row_test.path = "input_test.h5ad"
row_test.covar_col = np.nan
row_test.species = species

processed_test, num_cells, num_genes = process_raw_anndata(
    row=row_test,
    h5_folder_path=model_args["dir"],
    npz_folder_path=model_args["dir"],
    scp="",
    skip=model_args["skip"],
    additional_filter=model_args["filter"],
    root=model_args["dir"],
)
print(processed_test, flush=True)

print("\n>>> Generating test data indexes...", flush=True)
pe_row_idxs, dataset_chroms, dataset_pos = adata_path_to_prot_chrom_starts(
    processed_test, species, spec_pe_genes, gene_to_chrom_pos, offset
)
torch.save({model_args["name"]: pe_row_idxs}, model_args["pe_idx_path"])
with open(model_args["chroms_path"], "wb+") as f:
    pickle.dump({model_args["name"]: dataset_chroms}, f)
with open(model_args["starts_path"], "wb+") as f:
    pickle.dump({model_args["name"]: dataset_pos}, f)

print("\n>>> Embedding test data...", flush=True)
accelerator = Accelerator(project_dir=model_args["dir"])
accelerator.wait_for_everyone()
shapes_dict = {model_args["name"]: (num_cells, num_genes)}
run_eval(
    adata=processed_test,
    name=model_args["name"],
    pe_idx_path=model_args["pe_idx_path"],
    chroms_path=model_args["chroms_path"],
    starts_path=model_args["starts_path"],
    shapes_dict=shapes_dict,
    accelerator=accelerator,
    args=model_parameters,
)
embedded_test = ad.read_h5ad(os.path.join(model_args["dir"], "input_uce_adata.h5ad"))
print(embedded_test, flush=True)

print("\n>>> Classifying test data...", flush=True)
embedded_test.obs["label_pred"] = classifier.predict(embedded_test.obsm["X_uce"])

print("\n>>> Storing output...", flush=True)
output = ad.AnnData(
    obs=embedded_test.obs[["label_pred"]],
    uns={
        "method_id": meta["name"],
        "dataset_id": input_test.uns["dataset_id"],
        "normalization_id": input_test.uns["normalization_id"],
    },
)
print(output)

print("\n>>> Writing output AnnData to file...", flush=True)
output.write_h5ad(par["output"], compression="gzip")

print("\n>>> Cleaning up temporary directories...", flush=True)
work_dir.cleanup()
if model_temp is not None:
    model_temp.cleanup()

print("\n>>> Done!", flush=True)
